#
# This file contains the dataplane driver config. Multiple drivers can
# be specified and the 'default' driver must be specified. This is used
# when there is no better match found. The default driver should be last.
#
# The configurable fields per driver are:
# for each driver are:
# max_rxq=
#   The max number of rxqs to use for an interface of this type.
#   mandatory
#
# max_txq=
#   The max number of txqs to use for an interface of this type. This in only
#   used if not running in direct-transmit mode.
#   optional
#
# rx_desc=
#   The number of rx_buffers per queue with a max of 65k, should be a power of 2.
#   mandatory
#
# tx_desc=
#   The number of tx_buffers per queue with a max of 65k, should be a power of 2.
#   mandatory
#
# extra=
#   Extra buffers per rx queue, if required, with a max of 65k,
#   and should be a power of 2.
#   mandatory
#
# virtual=yes
#   A virtual device
#   optional
#
# limit-txq=yes
#   number of rxq == number of txq
#   optional
#
# disable_direct=yes
#   Do not do direct transmit. Always go through tx thread.
#
# tx_pkt_ring_size=
#   If running with separate txq, the size of the ring that the rx thread
#   enqueues to for tx processing. Default = 2048
#
# use_all_rxq=yes
#   Use all of the rxqs the interface is capable of using. Used for
#   devices that have no control over how many queues are used for RX
#   at runtime. When this is enabled max_rxq is ignored.
#   optional
#
# use_all_txq=yes
#   Use all of the txqs the interface is capable of using. Used for
#   devices that have no control over how many queues are used for TX
#   at runtime. When this is enabled max_txq is ignored.
#   optional
#

# In theory a 40G i/f should require 4x queues of 10G i/f,
# like ixgbe, so this should be 8. However, in practice PCIe
# 3.0 x8 transfer limits mean that line rate for 64byte
# packets isn't achievable, so give enough queues to support
# 128byte packets at line rate which is the max rate stated
# by the hardware tech docs presumably (although this likely
# includes some margin for error).
[i40e_40]
max_rxq=4
rx_desc=2048
tx_desc=128

[i40e_25]
max_rxq=4
rx_desc=2048
tx_desc=128

[i40e]
max_rxq=2
rx_desc=2048
tx_desc=128

[mlx4_100]
max_rxq=10
rx_desc=2048
tx_desc=128

[mlx4_40]
max_rxq=4
rx_desc=2048
tx_desc=128

[mlx4]
max_rxq=2
rx_desc=2048
tx_desc=128

[mlx5_100]
max_rxq=16
rx_desc=2048
tx_desc=1024
tx_desc_vm_multiplier=4
rx_offloads=keep_crc
tx_offloads=!dev_tx_offload_vlan_insert

[mlx5_40]
max_rxq=4
rx_desc=2048
tx_desc=128
tx_offloads=!dev_tx_offload_vlan_insert

[mlx5]
max_rxq=2
rx_desc=2048
tx_desc=128
tx_offloads=!dev_tx_offload_vlan_insert

[nicvf]
max_rxq=4
rx_desc=2048
tx_desc=128

[fm10k]
max_rxq=2
rx_desc=2048
tx_desc=128

# The ixgbe LSC interrupt is tells the driver that the link status
# is changing. The current DPDK driver schedules an event in the
# near future to check the status.  However, empircal testing has
# shown that the delay is too short. Instead of trying to pick
# some idea delay time, use the polling bheavior instead.
[ixgbe]
max_rxq=2
rx_desc=2048
tx_desc=512
dev_flags=!rte_eth_dev_intr_lsc

[bnx2x]
max_rxq=2
rx_desc=1024
tx_desc=128
limit-txq=yes

[bnxt]
max_rxq=5
rx_desc=2048
tx_desc=128

[vmxnet3]
max_rxq=2
rx_desc=512
tx_desc=512
extra=512
virtual=yes

[virtio]
max_rxq=2
rx_desc=256
tx_desc=256
virtual=yes
limit-txq=yes
rx_mq_mode=eth_mq_rx_none

[igb]
max_rxq=1
rx_desc=256
tx_desc=256

# use_all_txq needs to be set for the vhost PMD so that the
# dataplane has an equal number of TX and RX queues
[vhost]
rx_desc=256
tx_desc=256
virtual=yes
use_all_rxq=yes
use_all_txq=yes

# tx_desc for bonding needs to be >= DEFAULT ring size,
# or 4096, for vmxnet3 and <= MAX ring size, or 4096,
# for all drivers.
# 2048 rx_desc plus 2048 extra allocs 8192 mbufs.
# 8192+512(tx_desc)+2048(PKT_RING_SIZE), with
# DATAPLANE_SLAVE_MULTIPLIER, mbufs is 18944 and aligns
# to the next power of 2, it is 32768 mbufs.
#
# net_bonding can't advertise DEV_TX_OFFLOAD_MULTI_SEGS
# until after the first member is added. However, if we
# are able to configure the member it doesn't matter.
[bonding]
max_rxq=2
rx_desc=2048
tx_desc=512
extra=2048
virtual=yes
limit-txq=yes
tx_offloads=!dev_tx_offload_multi_segs

# limit-txq must be configured to yes because the unaccelerated
# version of the netvsc PMD assumes you will always have the
# same number of RX and TX queues.
[netvsc]
max_rxq=2
rx_desc=2048
tx_desc=2048
virtual=yes
limit-txq=yes

# unit testing only; no tx_offloads necessary.
[net_ring]
max_rxq=2
rx_desc=512
tx_desc=128
tx_offloads=!dev_tx_offload_multi_segs,!dev_tx_offload_vlan_insert

[cxgbe]
max_rxq=2
rx_desc=2048
tx_desc=512

[ena]
max_rxq=4
rx_desc=2048
tx_desc=512

[default]
max_rxq=2
rx_desc=512
tx_desc=128
